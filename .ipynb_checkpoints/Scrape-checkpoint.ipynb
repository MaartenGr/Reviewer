{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "\n",
    "\n",
    "def get_all_disney_titles() -> pd.DataFrame:\n",
    "    \"\"\" Get all Disney titles and their release dates \"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_Walt_Disney_Animation_Studios_films'\n",
    "    df = pd.read_html(url, header=0)[1]\n",
    "    df['Year'] = df.apply(lambda row: row['Release date'].split(\",\")[-1].strip(), 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_pixar_titles() -> pd.DataFrame:\n",
    "    \"\"\" Get all Pixar titles and their release dates \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_Pixar_films\"\n",
    "    df = pd.read_html(url, header=0)[0]\n",
    "    df = df.loc[df.Film != \"Released films\", :]\n",
    "    df = df.iloc[:22]\n",
    "    df['Year'] = df.apply(lambda row: row['Release date'].split(\",\")[-1].strip(), 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def match_years(search_result: Tag, year: str) -> bool:\n",
    "    \"\"\" Check if the year of a movie search matches (within 2 years) the year of the search result\"\"\"\n",
    "    string = search_result.text\n",
    "    year = int(year[-4:])\n",
    "    \n",
    "    # Extract year from string \n",
    "    string_year = re.sub('[^0-9]',' ', string)  # Keep numbers\n",
    "    string_year = re.sub(' +', ' ', string_year).strip()  # Remove duplicate whitespaces\n",
    "    string_year = int(string_year.split(\" \")[-1])\n",
    "    \n",
    "    if abs(string_year - year) <= 2:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def scrape_imdb_urls(df: pd.DataFrame) -> list:\n",
    "    \"\"\" Scrape IMDB urls of all the movies \"\"\"\n",
    "    titles = list(df.Film.values)\n",
    "    search_terms = (df.Film + \"%20\" + df.Year).values\n",
    "    urls = [None for _ in range(len(search_terms))]\n",
    "    \n",
    "    # Search for the movie and extract the first result\n",
    "    for index, search_term in tqdm(enumerate(search_terms)):\n",
    "\n",
    "        # Get search result page\n",
    "        search_url = f\"https://www.imdb.com/find?q={search_term}&s=tt&ttype=ft&ref_=fn_ft\"\n",
    "        res = requests.get(search_url).text\n",
    "        soup = BeautifulSoup(res,'lxml')\n",
    "\n",
    "        # Extract best search result\n",
    "        for result in soup.find_all(\"td\", class_=\"result_text\"):\n",
    "            if match_years(result, search_term):\n",
    "                url = result.find_all(\"a\", href=True)[0][\"href\"]\n",
    "                url = f\"https://www.imdb.com{url}reviews\"\n",
    "                urls[index] = url\n",
    "                break\n",
    "            \n",
    "    # Need to manually add saludos amigos as imdb's search engine cannot find it\n",
    "    urls = {title: url if url else \"https://www.imdb.com/title/tt0036326/reviews\" for url, title in zip(urls, titles)}\n",
    "    \n",
    "    # Also cannot find onward correctly...\n",
    "    if \"Onward\" in urls:\n",
    "        urls[\"Onward\"] = \"https://www.imdb.com/title/tt7146812/reviews\"\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_reviews(soup: BeautifulSoup):\n",
    "    \"\"\" Extract the title and reviews of a BeautifulSoup IMDB page \"\"\"\n",
    "    names = []\n",
    "    reviews = []\n",
    "    \n",
    "    for elem in soup.find_all(class_='imdb-user-review'):\n",
    "        name = elem.find(class_='title').get_text(strip=True)\n",
    "        names.append(name)\n",
    "        try:\n",
    "            review = elem.find(class_=\"content\").get_text(strip=True)\n",
    "            reviews.append(review)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return names, reviews\n",
    "\n",
    "\n",
    "def scrape_reviews(urls: str, driver_path: str = None) -> BeautifulSoup:\n",
    "    \"\"\" Scrape all reviews from a single movie on IMDB and return a soup instance\n",
    "    \n",
    "    It needs to use Chrome driver as the \"load more\" button should be \n",
    "    triggered multiple times in order to correctly load all reviews. \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    driver_path : str, default None\n",
    "        path to your chromedriver\n",
    "        \n",
    "    url : str\n",
    "        The url to scrape\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    soup : BeautifulSoup\n",
    "        A BeautifulSoup instance of the entire page\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_reviews = {title: [] for title in urls}\n",
    "    \n",
    "    for movie_title in urls:\n",
    "        \n",
    "        # Instantiate driver\n",
    "        driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Prepare page\n",
    "        driver.get(urls[movie_title])\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Press \"load more\" until the full page is loaded\n",
    "        # Curtosey of: https://stackoverflow.com/questions/55527423/why-do-i-only-get-first-page-data-when-using-selenium\n",
    "        while True:\n",
    "            try:\n",
    "                driver.find_element_by_css_selector(\"button#load-more-trigger\").click()\n",
    "                wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR,\".ipl-load-more__load-indicator\")))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            except Exception:break\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # Extract all reviews and their titles from the full page\n",
    "        titles, reviews = extract_reviews(soup)\n",
    "        all_reviews[movie_title] = [(review_title, movie_review) for review_title, movie_review in zip(titles, reviews)]\n",
    "\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney = get_all_disney_titles()\n",
    "pixar = get_all_pixar_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get imdb urls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:14,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# disney_urls = scrape_imdb_urls(disney)\n",
    "pixar_urls = scrape_imdb_urls(pixar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/disney_urls.json', 'w') as f:\n",
    "    json.dump(disney_urls, f)\n",
    "    \n",
    "with open('data/pixar_urls.json', 'w') as f:\n",
    "    json.dump(pixar_urls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrape reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixar_reviews = scrape_reviews(pixar_urls, \n",
    "                               '/home/CORP.VANSPAENDONCKGROEP.NL/maarten.grootendorst/Documents/Disney-NER/chromedriver')\n",
    "\n",
    "with open('data/pixar_reviews.json', 'w') as f:\n",
    "    json.dump(pixar_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_reviews = scrape_reviews(disney_urls, \n",
    "                                '/home/CORP.VANSPAENDONCKGROEP.NL/maarten.grootendorst/Documents/Disney-NER/chromedriver')\n",
    "\n",
    "with open('data/disney_reviews.json', 'w') as f:\n",
    "    json.dump(disney_reviews, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = {title: [] for title in urls}\n",
    "for movie_title in list(urls.keys())[:10]:\n",
    "    titles, reviews = scrape_reviews(urls[movie_title], \n",
    "                                     '/home/CORP.VANSPAENDONCKGROEP.NL/maarten.grootendorst/Documents/Disney-NER/chromedriver')\n",
    "    all_reviews[movie_title] = [(review_title, movie_review) for review_title, movie_review in zip(titles, reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://www.imdb.com/title/tt2294629/reviews\"  # frozen\n",
    "URL = \"https://www.imdb.com/title/tt2380307/reviews\"  # coco\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver,10)\n",
    "\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\"button#load-more-trigger\").click()\n",
    "        wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR,\".ipl-load-more__load-indicator\")))\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    except Exception:break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.find_element_by_class_name(\"ipl-expander\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "reviews = []\n",
    "for elem in soup.find_all(class_='imdb-user-review'):\n",
    "    name = elem.find(class_='title').get_text(strip=True)\n",
    "    names.append(name)\n",
    "    try:\n",
    "        review = elem.find(class_=\"content\").get_text(strip=True)\n",
    "        reviews.append(review)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('coco_reviews.json', 'w') as f:\n",
    "    json.dump(reviews, f)\n",
    "    \n",
    "with open('coco_review_titles.json', 'w') as f:\n",
    "    json.dump(names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Movie TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = [x.lower() for x in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = tfidf.fit_transform(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(X.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 10\n",
    "top_n = feature_array[tfidf_sorting][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['enough', 'me', 'had', 'smile', 'considering', 'reminded',\n",
       "       'sadness', 'related', 'tissues', 'grandma'], dtype='<U469')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Movies TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coco_reviews.json') as f:\n",
    "    coco = json.load(f)\n",
    "    \n",
    "with open('frozen_reviews.json') as f:\n",
    "    frozen = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = \" \".join(coco)\n",
    "frozen = \" \".join(frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\").fit([coco, frozen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = count.transform([coco, frozen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(t.todense()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377711, 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = t.sum(axis=0)\n",
    "m = 3000\n",
    "tf = np.divide(t,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tij = np.array(t.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(np.divide(m, sum_tij)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = np.multiply(tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sum = tf_idf.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = np.divide(tf_idf, unique_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(tf_idf, index=count.get_feature_names(), columns=[\"Coco\", \"Frozen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coco</th>\n",
       "      <th>Frozen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>coco</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>miguel</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dead</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pixar</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mexican</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>family</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>culture</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>permalink</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>helpful sign</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sign vote permalink</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sign vote</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>helpful sign vote</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>helpful review helpful</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>review helpful sign</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>helpful review</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>vote permalink</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>review helpful</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>vote</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sign</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.000849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>music</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Coco    Frozen\n",
       "coco                    0.004372  0.000002\n",
       "miguel                  0.004176  0.000000\n",
       "dead                    0.003984  0.000096\n",
       "pixar                   0.003784  0.000339\n",
       "mexican                 0.003375  0.000004\n",
       "family                  0.003152  0.000458\n",
       "culture                 0.003107  0.000085\n",
       "permalink               0.002899  0.000847\n",
       "helpful sign            0.002899  0.000847\n",
       "sign vote permalink     0.002899  0.000847\n",
       "sign vote               0.002899  0.000847\n",
       "helpful sign vote       0.002899  0.000847\n",
       "helpful review helpful  0.002899  0.000847\n",
       "review helpful sign     0.002899  0.000847\n",
       "helpful review          0.002899  0.000847\n",
       "vote permalink          0.002899  0.000847\n",
       "review helpful          0.002896  0.000848\n",
       "vote                    0.002894  0.000850\n",
       "sign                    0.002894  0.000849\n",
       "music                   0.002773  0.000918"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(\"Coco\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coco</th>\n",
       "      <th>Frozen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sister</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ice</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anna</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>olaf</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>snow</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>let</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.002089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>powers</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>songs</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.002051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hans</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>snowman</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>queen</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>princess</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>frozen</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.001973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>character</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>characters</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>song</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kristoff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>elsa</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>good</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>don</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coco    Frozen\n",
       "sister      0.000050  0.002330\n",
       "ice         0.000000  0.002210\n",
       "anna        0.000000  0.002154\n",
       "olaf        0.000166  0.002149\n",
       "snow        0.000014  0.002091\n",
       "let         0.000373  0.002089\n",
       "powers      0.000014  0.002083\n",
       "songs       0.000784  0.002051\n",
       "hans        0.000007  0.002025\n",
       "snowman     0.000007  0.002010\n",
       "queen       0.000007  0.001989\n",
       "princess    0.000022  0.001981\n",
       "frozen      0.000122  0.001973\n",
       "character   0.000841  0.001935\n",
       "characters  0.001070  0.001836\n",
       "song        0.000818  0.001808\n",
       "kristoff    0.000000  0.001785\n",
       "elsa        0.000001  0.001746\n",
       "good        0.001233  0.001734\n",
       "don         0.001022  0.001728"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(\"Frozen\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_en = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "# tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.json', 'w') as f:\n",
    "#     reviews = json.loads(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",       # Outside of a named entity\n",
    "    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "    \"I-MISC\",  # Miscellaneous entity\n",
    "    \"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "    \"I-PER\",   # Person's name\n",
    "    \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "    \"I-ORG\",   # Organisation\n",
    "    \"B-LOC\",   # Beginning of a location right after another location\n",
    "    \"I-LOC\"    # Location\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, sequence):\n",
    "    # Bit of a hack to get the tokens with the special tokens\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(inputs)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tokenizer_en, model_en, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CORP.VANSPAENDONCKGROEP.NL/maarten.grootendorst/.local/lib/python3.7/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CORP.VANSPAENDONCKGROEP.NL/maarten.grootendorst/.local/lib/python3.7/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# model_en = AutoModelForTokenClassification.from_pretrained(\"sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english\")\n",
    "# tokenizer_en = AutoTokenizer.from_pretrained(\"sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_en = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# tokenizer_en = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",       # Outside of a named entity\n",
    "    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "    \"I-MISC\",  # Miscellaneous entity\n",
    "    \"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "    \"I-PER\",   # Person's name\n",
    "    \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "    \"I-ORG\",   # Organisation\n",
    "    \"B-LOC\",   # Beginning of a location right after another location\n",
    "    \"I-LOC\"    # Location\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, sequence):\n",
    "    # Bit of a hack to get the tokens with the special tokens\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(inputs)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Marloes (ook een oud-collega) wil graag een keertje thee komen drinken en ik dacht dat dat wel handig is als jij werkt!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit predict(tokenizer_wietse, model_wietse, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.3 ms ± 5.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict(tokenizer_db, model_db, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "           \"close to the Manhattan Bridge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329 ms ± 46.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict(tokenizer_en, model_en, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, sequence):\n",
    "    # Bit of a hack to get the tokens with the special tokens\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(inputs)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    return predictions, tokens, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, tokens, outputs = predict(tokenizer_en, model_en, coco[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coco_reviews.json') as f:\n",
    "    coco = json.load(f)\n",
    "    \n",
    "with open('frozen_reviews.json') as f:\n",
    "    frozen = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Music is banned in Miguel's household after his musical great-grandfather abandoned his great-grandmother and her young daughter. On Mexico's Day Of The Dead festival, a convoluted series of events sees Miguel transported to the Land of the Dead, where he discovers that the dead disappear when no-one remembers them. He also discovers that unless he returns to the land of the living by dawn, he will join the dead. And there are problems ahead!The story here is very artificially structured, with all sorts of bits and pieces present s that a predetermined trail of events can be created. But you never notice that, so natural is the narrative flow.This film is highly original, full of truly gorgeous visuals, lovely songs, delightful characters, and touching resolutions to several individual story threads. And yet it appeared in the UK with no publicity, trailers, or warnings whatsoever, and my local multiscreen didn't even get it in 3D despite the fact that it is clear that the 3D would be dazzling.This is as good as any Pixar film over the last several years, and better than most.\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, tokens, _ = predict(tokenizer_en, model_en, \"Ernesto de la Cruz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Ernesto', 'de', 'la', 'Cruz', '[SEP]']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1095it [03:11,  5.71it/s]\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "for index, doc in tqdm(enumerate(coco)):\n",
    "    doc_names = []\n",
    "    \n",
    "    if len(doc.split(\" \")) < 300:\n",
    "        pred, tokens, _ = predict(tokenizer_en, model_en, doc)\n",
    "        pred = pred.tolist()[0]\n",
    "\n",
    "        for val in np.where(np.isin(pred,[3, 4]))[0]:\n",
    "            names.append(tokens[val])\n",
    "#     else:\n",
    "#         for sentence in doc.split(\".\"):\n",
    "#             if len(sentence.split(\" \")) < 300:\n",
    "#                 pred, tokens, _ = predict(tokenizer_en, model_en, sentence)\n",
    "#                 pred = pred.tolist()[0]\n",
    "\n",
    "#                 for val in np.where(np.isin(pred,[3, 4]))[0]:\n",
    "#                     names.append(tokens[val])\n",
    "            \n",
    "#     names.append((doc_names, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Pi': 160,\n",
       "         'Per': 676,\n",
       "         'Co': 332,\n",
       "         '##co': 162,\n",
       "         'Miguel': 206,\n",
       "         'X': 4,\n",
       "         'Shaw': 2,\n",
       "         '##xa': 89,\n",
       "         'Adrian': 12,\n",
       "         'Mo': 16,\n",
       "         '##lina': 12,\n",
       "         'Matthew': 4,\n",
       "         'Al': 4,\n",
       "         '##dric': 4,\n",
       "         '##h': 2,\n",
       "         'Lee': 20,\n",
       "         'Un': 21,\n",
       "         '##k': 19,\n",
       "         '##rich': 20,\n",
       "         'Anthony': 18,\n",
       "         'Gonzalez': 14,\n",
       "         'G': 19,\n",
       "         '##ael': 15,\n",
       "         'García': 7,\n",
       "         'Bern': 15,\n",
       "         'H': 5,\n",
       "         '##ctor': 6,\n",
       "         'Rivera': 9,\n",
       "         'Benjamin': 11,\n",
       "         'B': 12,\n",
       "         '##rat': 11,\n",
       "         '##t': 11,\n",
       "         'Ernesto': 47,\n",
       "         'de': 17,\n",
       "         'la': 20,\n",
       "         'Cruz': 34,\n",
       "         'Ana': 2,\n",
       "         'Of': 2,\n",
       "         '##eli': 3,\n",
       "         '##a': 5,\n",
       "         'Mu': 8,\n",
       "         '##rg': 2,\n",
       "         '##u': 2,\n",
       "         '##ía': 1,\n",
       "         'Ma': 1,\n",
       "         'So': 1,\n",
       "         '##cor': 1,\n",
       "         '##ro': 1,\n",
       "         'Don': 2,\n",
       "         'Jesus': 1,\n",
       "         'Disney': 33,\n",
       "         'Citizen': 2,\n",
       "         'Kane': 1,\n",
       "         'Dante': 12,\n",
       "         'Hector': 45,\n",
       "         'Trump': 4,\n",
       "         'Pedro': 2,\n",
       "         'In': 2,\n",
       "         '##fant': 2,\n",
       "         'Death': 1,\n",
       "         'I': 6,\n",
       "         '##mel': 3,\n",
       "         '##da': 3,\n",
       "         'Do': 2,\n",
       "         '##lby': 1,\n",
       "         'Garcia': 8,\n",
       "         '##al': 12,\n",
       "         'D': 2,\n",
       "         '##os': 7,\n",
       "         'Up': 2,\n",
       "         '##ana': 1,\n",
       "         'Jason': 3,\n",
       "         'Katz': 3,\n",
       "         'Ali': 1,\n",
       "         '##gh': 1,\n",
       "         '##ieri': 1,\n",
       "         'Di': 5,\n",
       "         'Michael': 2,\n",
       "         '##iac': 2,\n",
       "         '##chin': 2,\n",
       "         '##o': 4,\n",
       "         'Brave': 1,\n",
       "         'A': 1,\n",
       "         '##vila': 1,\n",
       "         'Lisa': 1,\n",
       "         'Ke': 1,\n",
       "         '##lle': 1,\n",
       "         '##her': 1,\n",
       "         'Elizabeth': 1,\n",
       "         'Dash': 1,\n",
       "         'Helen': 3,\n",
       "         'Tarzan': 1,\n",
       "         'Violet': 3,\n",
       "         'Edna': 1,\n",
       "         'Guillermo': 2,\n",
       "         'Del': 1,\n",
       "         'Toro': 2,\n",
       "         'Mi': 1,\n",
       "         '##chu': 1,\n",
       "         '##el': 1,\n",
       "         'Alan': 3,\n",
       "         '##na': 3,\n",
       "         'U': 3,\n",
       "         '##bach': 3,\n",
       "         'Edward': 2,\n",
       "         'James': 2,\n",
       "         'O': 3,\n",
       "         '##lm': 2,\n",
       "         'Go': 5,\n",
       "         '##nza': 5,\n",
       "         '##les': 4,\n",
       "         'Clint': 1,\n",
       "         'Eastwood': 1,\n",
       "         '##ki': 1,\n",
       "         '##rch': 1,\n",
       "         '##zel': 1,\n",
       "         '##az': 1,\n",
       "         'del': 3,\n",
       "         'Lindsey': 1,\n",
       "         'mi': 3,\n",
       "         'Op': 1,\n",
       "         'Pa': 1,\n",
       "         '##pp': 1,\n",
       "         '##ie': 1,\n",
       "         'De': 19,\n",
       "         'La': 13,\n",
       "         '##r': 7,\n",
       "         'Oscar': 2,\n",
       "         'Dad': 2,\n",
       "         'Sienna': 1,\n",
       "         'Sebastian': 1,\n",
       "         'Paul': 1,\n",
       "         'Oct': 1,\n",
       "         '##avi': 1,\n",
       "         'Paz': 1,\n",
       "         'Pan': 1,\n",
       "         '##och': 1,\n",
       "         '##udo': 1,\n",
       "         'Iris': 1,\n",
       "         'E': 2,\n",
       "         'Musa': 1,\n",
       "         '##fa': 2,\n",
       "         '##ti': 1,\n",
       "         'Ralph': 1,\n",
       "         'Mama': 6,\n",
       "         'Too': 1,\n",
       "         'Rolls': 1,\n",
       "         'Olaf': 3,\n",
       "         'Pac': 1,\n",
       "         'Lucia': 1,\n",
       "         'Elvis': 1,\n",
       "         'Presley': 1,\n",
       "         'Fr': 4,\n",
       "         'DE': 1,\n",
       "         '##é': 2,\n",
       "         'Jorge': 1,\n",
       "         'N': 1,\n",
       "         '##eg': 1,\n",
       "         '##ret': 1,\n",
       "         'Maria': 1,\n",
       "         'Felix': 1,\n",
       "         'Renee': 1,\n",
       "         'Victor': 1,\n",
       "         'Jaime': 1,\n",
       "         'Cam': 1,\n",
       "         '##il': 1,\n",
       "         'Alfonso': 1,\n",
       "         'Ara': 1,\n",
       "         'Herbert': 1,\n",
       "         'Si': 1,\n",
       "         '##gue': 1,\n",
       "         '##ui': 1,\n",
       "         'Se': 1,\n",
       "         '##lene': 1,\n",
       "         'Luna': 1,\n",
       "         'Sofia': 1,\n",
       "         '##sp': 1,\n",
       "         '##ino': 1,\n",
       "         '##sa': 1,\n",
       "         'Abu': 3,\n",
       "         'Papa': 2,\n",
       "         'Julio': 1,\n",
       "         'T': 3,\n",
       "         '##io': 1,\n",
       "         'Felipe': 1,\n",
       "         'R': 1,\n",
       "         '##ita': 1,\n",
       "         'Chi': 1,\n",
       "         '##cha': 1,\n",
       "         'Finding': 2,\n",
       "         'Rat': 1,\n",
       "         '##ato': 1,\n",
       "         'C': 5,\n",
       "         'Clement': 2,\n",
       "         'Moore': 2,\n",
       "         'Henry': 1,\n",
       "         'Livingston': 1,\n",
       "         'Jr': 1,\n",
       "         'Hank': 1,\n",
       "         '##lem': 1,\n",
       "         'Bo': 1,\n",
       "         'x': 1,\n",
       "         'P': 1,\n",
       "         'Walt': 1,\n",
       "         'John': 1,\n",
       "         'Wayne': 1,\n",
       "         'Los': 2,\n",
       "         '##ert': 2,\n",
       "         '##mal': 1,\n",
       "         'Marco': 1,\n",
       "         'Antonio': 1,\n",
       "         'Sol': 1,\n",
       "         '##is': 1,\n",
       "         'Mr': 1,\n",
       "         'Joy': 1,\n",
       "         'Mike': 1,\n",
       "         'W': 1,\n",
       "         '##oz': 1,\n",
       "         '##owski': 1,\n",
       "         'Du': 1,\n",
       "         'le': 1,\n",
       "         '##ru': 1,\n",
       "         '##lan': 1,\n",
       "         'g': 1,\n",
       "         '##rac': 2,\n",
       "         '##ias': 2,\n",
       "         'Em': 1,\n",
       "         '##er': 1,\n",
       "         '##la': 1,\n",
       "         'LA': 2,\n",
       "         'Lo': 1,\n",
       "         '##zo': 1,\n",
       "         'Lady': 1,\n",
       "         'Bird': 2,\n",
       "         'Brad': 1,\n",
       "         'Pete': 1,\n",
       "         'Doc': 1,\n",
       "         '##ter': 1,\n",
       "         'Andrew': 1,\n",
       "         'Stanton': 1,\n",
       "         'Flynn': 1,\n",
       "         'Art': 1,\n",
       "         '##father': 1,\n",
       "         'God': 1})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1119it [03:34,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "frozen_names = []\n",
    "for index, doc in tqdm(enumerate(frozen)):\n",
    "    \n",
    "    if len(doc.split(\" \")) < 300:\n",
    "        pred, tokens, _ = predict(tokenizer_en, model_en, doc)\n",
    "        pred = pred.tolist()[0]\n",
    "\n",
    "        for val in np.where(np.isin(pred,[3, 4]))[0]:\n",
    "            frozen_names.append(tokens[val])\n",
    "            \n",
    "#     names.append((doc_names, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Per': 370,\n",
       "         'Fr': 108,\n",
       "         'Men': 66,\n",
       "         '##zel': 66,\n",
       "         'Bell': 56,\n",
       "         'Olaf': 207,\n",
       "         'Disney': 231,\n",
       "         'Elsa': 459,\n",
       "         'Kris': 95,\n",
       "         '##to': 73,\n",
       "         'Hans': 102,\n",
       "         'Christian': 30,\n",
       "         'Anderson': 9,\n",
       "         'Sven': 39,\n",
       "         'Anna': 415,\n",
       "         'Pop': 1,\n",
       "         'En': 1,\n",
       "         'B': 3,\n",
       "         '##ly': 1,\n",
       "         '##ton': 1,\n",
       "         'Kristen': 51,\n",
       "         'Rocky': 1,\n",
       "         'Bull': 1,\n",
       "         '##win': 1,\n",
       "         'Sherman': 1,\n",
       "         'Walt': 21,\n",
       "         'El': 6,\n",
       "         '##za': 1,\n",
       "         'Male': 1,\n",
       "         '##ent': 1,\n",
       "         'C': 8,\n",
       "         'Andersen': 23,\n",
       "         'Ari': 1,\n",
       "         'F': 9,\n",
       "         '##olm': 1,\n",
       "         '##an': 1,\n",
       "         'Holly': 2,\n",
       "         'Al': 3,\n",
       "         'S': 9,\n",
       "         '##hr': 2,\n",
       "         '##ek': 1,\n",
       "         'Ce': 1,\n",
       "         '##line': 1,\n",
       "         'Dion': 1,\n",
       "         'Elton': 1,\n",
       "         'John': 4,\n",
       "         'I': 63,\n",
       "         '##dina': 63,\n",
       "         'Ra': 1,\n",
       "         'Good': 1,\n",
       "         'Po': 4,\n",
       "         '##ca': 1,\n",
       "         '##hon': 1,\n",
       "         '##tas': 2,\n",
       "         'Robert': 1,\n",
       "         'Lopez': 3,\n",
       "         'Prince': 16,\n",
       "         '##ff': 74,\n",
       "         'Pi': 16,\n",
       "         'The': 3,\n",
       "         'Grimm': 1,\n",
       "         'Jess': 1,\n",
       "         '##i': 2,\n",
       "         'Co': 1,\n",
       "         '##rt': 1,\n",
       "         '##din': 2,\n",
       "         'Jennifer': 19,\n",
       "         'Lee': 20,\n",
       "         '##se': 2,\n",
       "         'Ki': 2,\n",
       "         '##rsten': 1,\n",
       "         'Anders': 1,\n",
       "         'G': 50,\n",
       "         '##phe': 1,\n",
       "         'J': 5,\n",
       "         'Bin': 2,\n",
       "         'Hay': 1,\n",
       "         '##ao': 1,\n",
       "         'Mi': 3,\n",
       "         '##f': 2,\n",
       "         '##d': 1,\n",
       "         'Rap': 15,\n",
       "         '##is': 1,\n",
       "         'Buck': 13,\n",
       "         'Sant': 6,\n",
       "         '##ino': 6,\n",
       "         '##ont': 6,\n",
       "         '##ana': 6,\n",
       "         'Jonathan': 15,\n",
       "         '##ro': 14,\n",
       "         'Josh': 31,\n",
       "         '##ad': 29,\n",
       "         'Leonardo': 1,\n",
       "         'Di': 1,\n",
       "         '##C': 1,\n",
       "         '##ap': 1,\n",
       "         '##rio': 1,\n",
       "         'George': 2,\n",
       "         '##lo': 1,\n",
       "         '##oney': 1,\n",
       "         'Price': 1,\n",
       "         'Cinderella': 4,\n",
       "         '##tin': 6,\n",
       "         '##risto': 2,\n",
       "         'Chris': 15,\n",
       "         '##iara': 3,\n",
       "         '##n': 3,\n",
       "         'Hi': 3,\n",
       "         '##un': 3,\n",
       "         'Love': 1,\n",
       "         'Younger': 1,\n",
       "         'E': 2,\n",
       "         'T': 3,\n",
       "         'Beck': 3,\n",
       "         'Ana': 7,\n",
       "         'Eddie': 1,\n",
       "         'Murphy': 1,\n",
       "         'Bob': 1,\n",
       "         '##od': 2,\n",
       "         '##o': 3,\n",
       "         'Sam': 1,\n",
       "         'Las': 4,\n",
       "         '##set': 4,\n",
       "         'Mickey': 7,\n",
       "         'Amy': 1,\n",
       "         'Goodman': 1,\n",
       "         'Take': 1,\n",
       "         'Shakespeare': 1,\n",
       "         '##xa': 10,\n",
       "         'Summer': 1,\n",
       "         '##ya': 2,\n",
       "         '##zaki': 2,\n",
       "         'Mo': 3,\n",
       "         'Meg': 1,\n",
       "         '##yn': 1,\n",
       "         'Kelly': 1,\n",
       "         'God': 6,\n",
       "         'Eleanor': 2,\n",
       "         'Marianne': 3,\n",
       "         'Dash': 1,\n",
       "         '##wood': 1,\n",
       "         'Jane': 1,\n",
       "         'Austen': 1,\n",
       "         'Emma': 1,\n",
       "         'Thompson': 1,\n",
       "         'Kate': 1,\n",
       "         'Win': 1,\n",
       "         '##sle': 1,\n",
       "         '##t': 1,\n",
       "         'OH': 1,\n",
       "         'O': 4,\n",
       "         '##ken': 2,\n",
       "         'Brave': 5,\n",
       "         'Sofia': 1,\n",
       "         'Christophe': 2,\n",
       "         'De': 8,\n",
       "         '##mi': 7,\n",
       "         'Lo': 6,\n",
       "         '##vat': 7,\n",
       "         'Han': 5,\n",
       "         'Aren': 9,\n",
       "         'Oscar': 1,\n",
       "         '##mal': 4,\n",
       "         '##w': 1,\n",
       "         '##hoe': 1,\n",
       "         'Judy': 1,\n",
       "         'Garland': 1,\n",
       "         'Ha': 1,\n",
       "         '##rns': 1,\n",
       "         'Alan': 4,\n",
       "         'Akira': 1,\n",
       "         'Fe': 1,\n",
       "         '##lidae': 1,\n",
       "         'Snow': 12,\n",
       "         'White': 7,\n",
       "         'Tarzan': 5,\n",
       "         'Oro': 1,\n",
       "         'Munro': 1,\n",
       "         '##e': 1,\n",
       "         'Storm': 1,\n",
       "         'Is': 2,\n",
       "         'Adult': 1,\n",
       "         'Flynn': 7,\n",
       "         'Bugs': 1,\n",
       "         'Bunny': 1,\n",
       "         'Pete': 1,\n",
       "         'le': 1,\n",
       "         'Tu': 3,\n",
       "         '##dy': 3,\n",
       "         'Ai': 4,\n",
       "         '##sha': 4,\n",
       "         'Hank': 2,\n",
       "         '##á': 1,\n",
       "         '##bio': 1,\n",
       "         'el': 1,\n",
       "         'For': 1,\n",
       "         'Anne': 1,\n",
       "         '##dell': 4,\n",
       "         'An': 1,\n",
       "         'PD': 1,\n",
       "         'Kay': 1,\n",
       "         'Mu': 4,\n",
       "         '##lan': 3,\n",
       "         'Oak': 1,\n",
       "         '##zal': 1,\n",
       "         'Glad': 1,\n",
       "         '##as': 1,\n",
       "         'Eli': 2,\n",
       "         '##sa': 1,\n",
       "         'V': 1,\n",
       "         'He': 1,\n",
       "         '##im': 1,\n",
       "         'À': 1,\n",
       "         '##rna': 1,\n",
       "         'Si': 1,\n",
       "         'Liv': 1,\n",
       "         '##vy': 1,\n",
       "         'St': 2,\n",
       "         '##uben': 1,\n",
       "         '##ra': 1,\n",
       "         '##lar': 1,\n",
       "         '##g': 1,\n",
       "         'Frost': 1,\n",
       "         'Sleeping': 1,\n",
       "         'Christ': 3,\n",
       "         '##en': 1,\n",
       "         '##te': 1,\n",
       "         'K': 2,\n",
       "         'Grandpa': 2,\n",
       "         '##bb': 1,\n",
       "         '##ie': 2,\n",
       "         'Len': 1,\n",
       "         'Temple': 1,\n",
       "         'Grand': 2,\n",
       "         '##in': 1,\n",
       "         'Ko': 1,\n",
       "         '##di': 1,\n",
       "         'Me': 2,\n",
       "         '##rida': 1,\n",
       "         'Robin': 1,\n",
       "         'Williams': 1,\n",
       "         '##ov': 1,\n",
       "         '##ot': 1,\n",
       "         'Tang': 1,\n",
       "         'Ren': 1,\n",
       "         'Blu': 1,\n",
       "         'Ray': 1,\n",
       "         'Ball': 1,\n",
       "         'Her': 1,\n",
       "         '##mio': 1,\n",
       "         '##ne': 2,\n",
       "         'Emilia': 1,\n",
       "         '##ar': 3,\n",
       "         '##hart': 1,\n",
       "         'Marie': 1,\n",
       "         '##uri': 1,\n",
       "         'Cleopatra': 1,\n",
       "         '##nds': 2,\n",
       "         'Pa': 2,\n",
       "         '##Z': 1,\n",
       "         'Joan': 1,\n",
       "         'Jamie': 1,\n",
       "         'Curtis': 1,\n",
       "         'Randy': 1,\n",
       "         'Newman': 1,\n",
       "         'Bravo': 2,\n",
       "         'Ba': 3,\n",
       "         'Christoph': 2,\n",
       "         'Patrick': 1,\n",
       "         'Star': 1,\n",
       "         'N': 1,\n",
       "         '##em': 1,\n",
       "         'A': 1,\n",
       "         '##erial': 1,\n",
       "         '##ks': 1,\n",
       "         'Amanda': 1,\n",
       "         'By': 1,\n",
       "         '##nes': 1,\n",
       "         'In': 2,\n",
       "         'Ariel': 1,\n",
       "         'Ed': 1,\n",
       "         '##ina': 1,\n",
       "         '##nn': 1,\n",
       "         '##och': 1,\n",
       "         '##io': 1,\n",
       "         '##mb': 1,\n",
       "         'Ann': 1,\n",
       "         'Santa': 4,\n",
       "         'Nikolai': 1,\n",
       "         'Rudolph': 1,\n",
       "         'Cat': 2,\n",
       "         'Art': 3,\n",
       "         'Agatha': 1,\n",
       "         'Mon': 1,\n",
       "         'Katie': 1,\n",
       "         '##ik': 1,\n",
       "         'Hu': 1,\n",
       "         '##mber': 1,\n",
       "         'na': 1,\n",
       "         'Barbie': 2,\n",
       "         '##li': 1,\n",
       "         'Ralph': 1,\n",
       "         '##bet': 1,\n",
       "         '##tien': 1,\n",
       "         'La': 1,\n",
       "         'Kurt': 1,\n",
       "         'Von': 1,\n",
       "         '##gu': 1,\n",
       "         'Veronica': 1,\n",
       "         'Mars': 1,\n",
       "         'Matthew': 1,\n",
       "         '##rod': 1,\n",
       "         '##erick': 1,\n",
       "         'James': 2,\n",
       "         'Earl': 1,\n",
       "         'Jones': 1,\n",
       "         'Nathan': 1,\n",
       "         'Lane': 1,\n",
       "         'Maria': 2,\n",
       "         '##ou': 2,\n",
       "         '##nous': 2,\n",
       "         'Do': 1,\n",
       "         '##uche': 1,\n",
       "         '##erd': 1,\n",
       "         'Mouse': 1,\n",
       "         'Wilson': 1,\n",
       "         'Tri': 1,\n",
       "         '##vin': 1,\n",
       "         'Mother': 1,\n",
       "         '##rst': 1,\n",
       "         '##off': 1,\n",
       "         '##rid': 1,\n",
       "         'Marc': 1,\n",
       "         'Lab': 1,\n",
       "         '##re': 1,\n",
       "         'Bobby': 1,\n",
       "         '##er': 1,\n",
       "         'P': 1,\n",
       "         'L': 1,\n",
       "         '##top': 1,\n",
       "         '##h': 1,\n",
       "         'R': 1,\n",
       "         '##nde': 1,\n",
       "         'Gray': 1,\n",
       "         'Alfred': 1,\n",
       "         'Hitchcock': 1,\n",
       "         'Mae': 1,\n",
       "         'Cut': 1,\n",
       "         '##k': 1,\n",
       "         'Duke': 1,\n",
       "         'Igor': 1})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(frozen_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
